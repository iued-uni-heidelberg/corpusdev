{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpusdev/blob/main/S104SentenceAlignmentHunalignWSentenceTokeniserV02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments with alignment tools"
      ],
      "metadata": {
        "id": "hhDdKnVbT6Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "resources / corpora:\n",
        "https://heibox.uni-heidelberg.de/d/d65daff8341e467c82b1/\n",
        "\n",
        "\n",
        "Instructions are here:\n",
        "https://github.com/danielvarga/hunalign\n"
      ],
      "metadata": {
        "id": "iM-SQNgcjh6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wTTt2yCTo7U"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielvarga/hunalign.git\n",
        "%cd hunalign/src/hunalign\n",
        "!pwd\n",
        "!make\n",
        "%cd /content/hunalign/\n",
        "!pwd\n",
        "!src/hunalign/hunalign data/hu-en.stem.dic examples/demo.hu.stem examples/demo.en.stem -hand=examples/demo.manual.ladder -text > /tmp/align.txt\n",
        "!head --lines=5 /tmp/align.txt\n",
        "%cd /content/\n",
        "!pwd\n",
        "\n",
        "# detecting sentence boundaries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import re, os, sys\n",
        "def sentTokenizer(FIn, FOut):\n",
        "    for SPara in FIn:\n",
        "        SPara = SPara.strip()\n",
        "        if SPara == '': FOut.write('\\n')\n",
        "        all_sent = sent_tokenize(SPara)\n",
        "        for SSent in all_sent:\n",
        "            FOut.write(SSent + '\\n')\n",
        "    FOut.flush()\n",
        "\n",
        "def sentTokenizer_hy(FIn, FOut):\n",
        "    for SPara in FIn:\n",
        "        SPara = SPara.strip()\n",
        "        if SPara == '': \n",
        "            FOut.write('\\n')\n",
        "            continue\n",
        "        all_sent = sent_tokenize_hy(SPara)\n",
        "        for SSent in all_sent:\n",
        "            FOut.write(SSent + '\\n')\n",
        "    FOut.flush()\n",
        "\n",
        "\n",
        "def sent_tokenize_hy(SPara):\n",
        "    all_sent_hy = re.split('[:;]', SPara)\n",
        "    if isinstance(all_sent_hy, list):\n",
        "        return all_sent_hy\n",
        "    else:\n",
        "        return []\n",
        "    \n",
        "def sent_tokenize_en(SPara):\n",
        "    all_sent_hy = re.split('[\\.\\!\\?]', SPara)\n",
        "    if isinstance(all_sent_hy, list):\n",
        "        return all_sent_hy\n",
        "    else:\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing automatically uploaded files"
      ],
      "metadata": {
        "id": "3V9Imvq9_Ond"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional : try with automatically downloaded files\n",
        "\n",
        "# downloading files - en de\n",
        "!wget https://heibox.uni-heidelberg.de/f/dc489828a2d642bfba82/?dl=1\n",
        "!mv index.html?dl=1 ab-t01-en.txt\n",
        "# English file\n",
        "!wget https://heibox.uni-heidelberg.de/f/e7e542d75d7644d1857d/?dl=1\n",
        "!mv index.html?dl=1 ab-t01-de.txt\n",
        "\n",
        "\n",
        "FIn = open('ab-t01-en.txt', 'r')\n",
        "FOut = open('ab-t01s-en.txt', 'w')\n",
        "# FIn = open('ab-t01-de.txt', 'r')\n",
        "# FOut = open('ab-t01s-de.txt', 'w')\n",
        "sentTokenizer(FIn, FOut)\n",
        "\n",
        "FIn = open('ab-t01-de.txt', 'r')\n",
        "FOut = open('ab-t01s-de.txt', 'w')\n",
        "sentTokenizer(FIn, FOut)\n",
        "\n",
        "# !./hunalign/src/hunalign/hunalign -text -realign -autodict=de2enAutodict.txt ./hunalign/data/null.dic ab-t01-en.txt ab-t01-de.txt > ab-t01-en2de.tsv\n",
        "!./hunalign/src/hunalign/hunalign -text -realign -autodict=de2enAutodict.txt ./hunalign/data/null.dic ab-t01s-en.txt ab-t01s-de.txt > ab-t01s-en2de.tsv"
      ],
      "metadata": {
        "id": "OYDn_STYYa1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload two files from Moodle:\n",
        "- vaccination-safety-t101-de.txt\n",
        "- vaccination-safety-t101-hy.txt"
      ],
      "metadata": {
        "id": "4mP0qWlq9BY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional \n",
        "!head --lines=20 vaccination-safety-t101-de.txt"
      ],
      "metadata": {
        "id": "HWcpViE4ecG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional \n",
        "!head --lines=20 vaccination-safety-t101-hy.txt"
      ],
      "metadata": {
        "id": "65s_UZhweilW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "FIn1 = open('vaccination-safety-t101-hy.txt')\n",
        "FIn2 = open('vaccination-safety-t101-de.txt')\n",
        "FOut1 = open('vaccination-safety-t101sent-hy.txt', 'w')\n",
        "FOut2 = open('vaccination-safety-t101sent-de.txt', 'w')\n",
        "sentTokenizer_hy(FIn1, FOut1)\n",
        "sentTokenizer(FIn2, FOut2)\n",
        "\n",
        "FOut1.close()\n",
        "FOut2.close()\n",
        "# you can edit dictionaries and try realigning tests if results are not good, like in this example:\n",
        "!./hunalign/src/hunalign/hunalign -text -realign -autodict=hy2deAutodict2.txt ./hunalign/data/null.dic /content/vaccination-safety-t101sent-hy.txt /content/vaccination-safety-t101sent-de.txt > vaccination-safety-t101-hy-de.txt\n",
        "%cd /content/\n",
        "## prepare for Phrase.com / TM\n",
        "FOut3 = open('vaccination-safety-t101-hy-de.csv.txt', 'w')\n",
        "FOut3.write('hy\\tde\\n')\n",
        "with open('vaccination-safety-t101-hy-de.txt') as FParallel:\n",
        "    for SLine in FParallel:\n",
        "        SLine = SLine.strip()\n",
        "        LLine = SLine.split('\\t')\n",
        "        if len(LLine) > 1:\n",
        "            SSrcLang = LLine[0]\n",
        "            STrgLang = LLine[1]\n",
        "            FOut3.write(f'{SSrcLang}\\t{STrgLang}\\n')\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    FOut3.flush()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i35YQ9WTZilz",
        "outputId": "2cba2a17-eb88-440f-c8a3-ab4fc6c909f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dictionary...\n",
            "269 source language sentences read.\n",
            "229 target language sentences read.\n",
            "quasiglobal_stopwordRemoval is set to 0\n",
            "Simplified dictionary ready.\n",
            "Rough translation ready.\n",
            "0 \n",
            "100 200 \n",
            "Rough translation-based similarity matrix ready.\n",
            "Matrix built.\n",
            "Trail found.\n",
            "Align ready.\n",
            "Global quality of unfiltered align 0.496832\n",
            "5112 items inside the border.\n",
            "Border of realign zone determined.\n",
            "184 bisentences collected.\n",
            "Plausible bisentences filtered.\n",
            "Removing stopwords...Removing identicals... (AstraZeneca) (Moderna) (Novavax) (Pfizer) 020 0481 080 1 11 131 14 15 16 1800 2 3 4 450 49 5 50 6 611 63 643 653 7 787 809 Clinic Comirnaty Disability Express Gateway Goods Nuvaxovid Plus Services Spikevax Therapeutic Vaccine Vaxzevria mobile â€“ \n",
            "44 identical translations found.\n",
            "Removing hapaxes...10 hapax-based dictionary items found.\n",
            "Building CorpusConstellation... Done.\n",
            "0 items left in original dictionary.\n",
            "Removing stopwords...Removing identicals... \n",
            "0 identical translations found.\n",
            "Removing hapaxes...0 hapax-based dictionary items found.\n",
            "Building CorpusConstellation... Done.\n",
            "248 new dictionary items found.\n",
            "Simplified dictionary ready.\n",
            "Rough translation ready.\n",
            "0 \n",
            "100 200 Matrix built.\n",
            "Trail found.\n",
            "Detail realign ready.\n",
            "Global quality of unfiltered align after realign 0.643761\n",
            "quasiglobal_spaceOutBySentenceLength is set to 1\n",
            "Trail spaced out by sentence length.\n",
            "Global quality of unfiltered align after realign 0.643761\n",
            "Quality 0.643761\n",
            "/content\n"
          ]
        }
      ]
    }
  ]
}