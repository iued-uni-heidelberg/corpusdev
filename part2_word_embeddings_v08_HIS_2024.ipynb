{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpusdev/blob/main/part2_word_embeddings_v08_HIS_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty2YWcLcvKl4"
      },
      "source": [
        "# Word Vectors\n",
        "\n",
        "Presentations [slides](intro-word-vectors-DAAD-2021-v22.pdf); Original [slides](word-vectors.pdf)\n",
        "\n",
        "Word vectors (also known as 'word embeddings') are one of the most popular kinds of AI models. They are extremely useful in many domains. In essence, a word vector is a set of numbers that attempt to capture the meaning of a word. In typical implementations, each word is represented by a set of 200-300 numbers. In linear algebra, a one-dimensional array of numbers is known as a 'vector', hence these sets of numbers representing words' meanings are known as 'word vectors'.\n",
        "\n",
        "Using neural networks, we can expose the computer to a large amount of text, and allow it to learn an appropriate set of numbers for each word it encounters. In this notebook, we will learn about the most famous of all word vector algorithms, `word2vec`, which was first described by Tomas Mikolov and his team in 2013:\n",
        "\n",
        "* Tomas Mikolov, Ilya Sutskever, and others, ‘Distributed Representations of Words and Phrases and Their Compositionality’, in Advances in Neural Information Processing Systems 26, ed. by C. J. C. Burges and others (Curran Associates, Inc., 2013), pp. 3111–19 <http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>\n",
        "* Tomas Mikolov, Kai Chen, and others, ‘Efficient Estimation of Word Representations in Vector Space’, ArXiv:1301.3781 Cs, 2013 <http://arxiv.org/abs/1301.3781>.\n",
        "\n",
        "In fact, `word2vec` is not a single algorithm, but rather a family of similar algorithms. In this session we will consider just the most famous `word2vec` algorithm, namely the `skip-gram model` trained using `negative sampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWLKKQ0lvKl5"
      },
      "source": [
        "## Applications of Word Vectors\n",
        "\n",
        "Word vectors allow the computer to 'understand' language far more effectively. Rather than seeing each word as simply an arbitrarily different object, a computer using word vectors can analyse each word as a point in 200- or 300-dimenstional space. Words that are similar in meaning will have similar word vectors. And as we will see, the spaces between the word vectors are also significant: the words are arranged in patterns that represent their relationships to one another.\n",
        "\n",
        "Accordingly, most AI systems that process language now include a word vector layer as part of their architecure. When the system encounters some text (e.g. when you speak to Siri or Alexa), your words are converted into word vectors, *and then* the computer examines what the text says and determines how it should respond.\n",
        "\n",
        "In the Humanities, word vectors have become a popular modelling tool, because they allow researchers to perform sophisticated analysis on large corpora of text. Some examples include:\n",
        "\n",
        "* [The Women Writers Vector Toolkit](https://wwp.northeastern.edu/lab/wwvt/index.html)\n",
        "* William L. Hamilton, Jure Leskovec, and Dan Jurafsky, ‘Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change’, ArXiv:1605.09096 [Cs], 2018 <http://arxiv.org/abs/1605.09096>.\n",
        "* Ryan Heuser, 'Semantic Networks' <https://ryanheuser.org/word-vectors-4/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ZWncLevKl5"
      },
      "source": [
        "## Training a `word2vec` model in Gensim\n",
        "\n",
        "\n",
        "It is very easy to train a `word2vec` model in Gensim, which includes Mikolov's original `word2vec` code in its codebase.\n",
        "\n",
        "### Step 0: Downloading texts\n",
        "#### 0.1: load an existing text collection...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgedLV80vKl6"
      },
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data\n",
        "\n",
        "# api.info()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = api.load('text8')\n",
        "word_vectors = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "7R3lyA2LRNkP",
        "outputId": "e4e19de9-7ef2-42a5-8019-7481a4f46684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[====----------------------------------------------] 9.4% 35.2/376.1MB downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "PzhPqQ6ARNVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvSxGbYN8aHr"
      },
      "source": [
        "text8 is first 100MB from English wikipedia, 17 million words\n",
        "\n",
        "The file is downloadable from\n",
        "http://mattmahoney.net/dc/text8.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOfGR4GVxzIY"
      },
      "source": [
        "# Examine the corpus to see what is there\n",
        "api.info(\"text8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNRnUeEDzJS8"
      },
      "source": [
        "type(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlSD09Qoz_6t"
      },
      "source": [
        "data = [d for d in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyeDslt-26E2"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUQzkxNbyTEF"
      },
      "source": [
        "len(data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[1])"
      ],
      "metadata": {
        "id": "PDK2-42fbCvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[2])"
      ],
      "metadata": {
        "id": "y8pK5IugbEyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEmirZe2yR9y"
      },
      "source": [
        "print(data[0][:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkvbCylDyZF_"
      },
      "source": [
        "print(data[1][:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[2][:100])"
      ],
      "metadata": {
        "id": "LNOIgD67aq-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 0.2: load own text collection to be used in training ... (will be added later)...\n"
      ],
      "metadata": {
        "id": "L46Z0Uw6AOuu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eEjBCxovKl6"
      },
      "source": [
        "### Step 1: Set hyperparameters and instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9MC27DDvKl6"
      },
      "source": [
        "my_vector_size = 100 # Dimensionality of the word vectors\n",
        "window = 5 # How many words either side? (5 = 5 context words either side, i.e. 10 context words in total)\n",
        "use_skip_gram = 1 # If you set this to 0, then it will create a 'continuous bag of words' model instead\n",
        "use_softmax = 0 # If you set this to 1, then hierarchical softmax will be used instead of negative sampling\n",
        "negative_samples = 5 # How many incorrect answers to generate per correct answer when negative sampling\n",
        "\n",
        "model = Word2Vec(\n",
        "    vector_size=my_vector_size,\n",
        "    window=window,\n",
        "    sg=use_skip_gram,\n",
        "    hs=use_softmax,\n",
        "    negative=negative_samples\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofhbkdEjvKl7"
      },
      "source": [
        "### Step 2: Fit model to corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEieLoWlvKl7"
      },
      "source": [
        "# build a model\n",
        "model.build_vocab(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e_ijem9vKl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a39850d-a52e-4bc0-a082-c6cfa9a0b00e"
      },
      "source": [
        "# Train the model on the corpus\n",
        "# model.train(sentences=corpus, epochs=5, total_examples=model.corpus_count)\n",
        "# runs approximately 10 minutes with 5 epochs...\n",
        "model.train(corpus, epochs=5, total_examples=model.corpus_count)\n",
        "\n",
        "# saving the model, zip can be downloaded...\n",
        "model.save(\"word2vec5e.model\")\n",
        "!zip word2vec5e.model.zip word2vec5e.model\n",
        "!rm word2vec5e.model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62529375, 85026035)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modelOwn.train(sentences=corpusOwn, epochs=5, total_examples=modelOwn.corpus_count)"
      ],
      "metadata": {
        "id": "fMhUGhj35OB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## !rm word2vec.model"
      ],
      "metadata": {
        "id": "mRUHn7Rmjw2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: alternative: downloading a pre-trained model"
      ],
      "metadata": {
        "id": "oWmzJCI8byY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Downloading and connecting pre-trained model\n",
        "!wget https://heibox.uni-heidelberg.de/f/b7625cd746534aacbefe/?dl=1\n",
        "!mv index.html?dl=1 word2vec.model"
      ],
      "metadata": {
        "id": "YGoYTjhmh1tL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057b01ec-a61a-4d4d-c15d-0418880efa9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 14:38:38--  https://heibox.uni-heidelberg.de/f/b7625cd746534aacbefe/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/2dc0b8fa-7013-495a-bf24-9bd188c1ce72/word2vec.model [following]\n",
            "--2024-06-09 14:38:38--  https://heibox.uni-heidelberg.de/seafhttp/files/2dc0b8fa-7013-495a-bf24-9bd188c1ce72/word2vec.model\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59217897 (56M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  56.47M  17.0MB/s    in 3.5s    \n",
            "\n",
            "2024-06-09 14:38:42 (16.2 MB/s) - ‘index.html?dl=1’ saved [59217897/59217897]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "# model2 = Word2Vec.load(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "mpMpt2SPkI0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThYRouIevKl8"
      },
      "source": [
        "### Step 3: Extract word vectors from model\n",
        "\n",
        "The fully trained model includes all of the weights used to predict the context words for each input word. If you are not planning on training the model further, these weights can be discarded, and you can just keep the weights for the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V0LfNx_vKl8"
      },
      "source": [
        "word_vectors = model.wv\n",
        "del model # Delete the whole model to free up the computer's RAM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.save(\"word2vec.wordvectors5e\")\n",
        "!zip word2vec.wordvectors5e.zip word2vec.wordvectors5e\n",
        "!rm word2vec.wordvectors5e"
      ],
      "metadata": {
        "id": "n-RuljZPkZno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d830071a-494d-4f6e-e294-dcb6297dad39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: word2vec.wordvectors5e (deflated 12%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## !rm word2vec.wordvectors"
      ],
      "metadata": {
        "id": "4G0KFEMtldFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 alternative: download and connect the pre-trained word vectors"
      ],
      "metadata": {
        "id": "9KP5wiumclM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/e7725b1afab9456c8e16/?dl=1\n",
        "!mv index.html?dl=1 word2vec.wordvectors"
      ],
      "metadata": {
        "id": "UeFHDzcWlV9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f1731b-3ab1-4671-c8d2-2da7b49c88ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 14:27:03--  https://heibox.uni-heidelberg.de/f/e7725b1afab9456c8e16/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/987d80a8-aee6-4f3f-b788-e95bb1a6fe27/word2vec.wordvectors [following]\n",
            "--2024-06-09 14:27:04--  https://heibox.uni-heidelberg.de/seafhttp/files/987d80a8-aee6-4f3f-b788-e95bb1a6fe27/word2vec.wordvectors\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30697035 (29M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  29.27M  15.8MB/s    in 1.9s    \n",
            "\n",
            "2024-06-09 14:27:06 (15.8 MB/s) - ‘index.html?dl=1’ saved [30697035/30697035]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
      ],
      "metadata": {
        "id": "1Ts09s5Plp86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 alternative-2: downloading a pre-trained Gigaword model, 300 dimensions... (~5 min)"
      ],
      "metadata": {
        "id": "6jJErlIMH7Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_vectors = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "1w9Xq0WHH3HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a9f621-5278-4d9f-98b6-2bfeaf37860f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: exploring word vectors"
      ],
      "metadata": {
        "id": "QKEJDhU8c6dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = word_vectors['computer']"
      ],
      "metadata": {
        "id": "5eDztGwqmJ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector"
      ],
      "metadata": {
        "id": "yt5Sah91ms2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSkZD22AvKl8"
      },
      "source": [
        "### Step 4: Have a play with the model\n",
        "\n",
        "There are several ways you can use word vectors. One of the most famous is to use them to compute analogies. The formula is:\n",
        "\n",
        "<center><em>x</em> is to <em>small</em> as <em>biggest</em> is to <em>big</em></center>\n",
        "\n",
        "$$x - vector('small') = vector('biggest') - vector('big')$$\n",
        "\n",
        "$$\\therefore x = vector('small') + vector('biggest') - vector('big')$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pftv0DEvKl8"
      },
      "source": [
        "# See the word vector for a particular word\n",
        "vector = word_vectors['banana']\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_qUSF7ZvKl9"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('university', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T38xLSz62OyK"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('red', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word_vectors.most_similar('color', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "metadata": {
        "id": "mMnUGzMsF3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word_vectors.most_similar('colour', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "metadata": {
        "id": "U53t6hXxF6x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvyAmb152SMD"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('brussels', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('heidelberg', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "metadata": {
        "id": "tSF2dmZyGE2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('international', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "metadata": {
        "id": "oNXsAP-aGNjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r68SeidU2lRz"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('germany', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RVNTkIs2thF"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors.most_similar('strange', topn=25)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oqwf3ni21zW"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors.most_similar(negative=['man'], positive=['king','woman'], topn=20)\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sVv80bYvKl9"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors.most_similar(negative=['king'], positive=['queen','man'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8gfHhZf3Byi"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors.most_similar(negative=['mother'], positive=['father','daughter'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuzURZa_6hQC"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors.most_similar(negative=['mother'], positive=['father','girl'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzzlKdei6uiW"
      },
      "source": [
        "# Compute analogous words\n",
        "# x is to daughter as people is to person (plural + daughter)\n",
        "analogous_words = word_vectors.most_similar(negative=['person'], positive=['people','daughter'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['soft'], positive=['hard','softer'])\n",
        "# analogous_words = word_vectors.most_similar(negative=['soft'], positive=['hard','softest'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "EO2Frm6xoPKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['soft'], positive=['hard','softest'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "B9injYVJR6TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['doyle'], positive=['holmes','poirot'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "ZcVS0sUMKNAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['sherlock'], positive=['holmes','poirot'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "LwXkDhnMLYOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['blue'], positive=['green', 'red'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "KHuZ44b9Ln-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['sparrow'], positive=['bird', 'labrador']) # bulldog, chair, labrador\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "ryMjz-HvPKGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = word_vectors.most_similar(negative=['herd'], positive=['sheep', 'flock'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "0uroriYCPK74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22fSc-iyvKl9"
      },
      "source": [
        "## Step 4A - optional. Using pre-trained models in Gensim\n",
        "\n",
        "In many applications, you will simply want access to pre-trained word vectors (e.g. for plugging in to another model you are training). If you don't need the vectors to be tailored closely to your particular corpus, then you might like to use some pretrained models.\n",
        "\n",
        "`word2vec` is not the only word embedding family of algorithms. Another, arguably even more powerful algorithm is the `FastText` algorithm, which Mikolov developed after moving to Facebook:\n",
        "\n",
        "* Piotr Bojanowski and others, ‘Enriching Word Vectors with Subword Information’, ArXiv:1607.04606, 2017 <http://arxiv.org/abs/1607.04606>.\n",
        "\n",
        "Instead of computing word vectors for each word, FastText splits each word into its constituent chunks. For example, 'cat' would be split into 'c', 'a', 't', 'ca', 'at' and 'cat', and 'burp' would be split into 'b', 'u', 'r', 'p', 'bu', 'ur', 'rp', 'bur', 'urp' and 'burp'. Then a vector is computer for each chunk that appears in the corpus. Each word is represented as the mean of all the chunks that make it up. FastText is able to learn very good word vectors because it can extract meaning from subword units, e.g. it can see that 'television', 'telegraph' and 'telepathy' all have 'tele' at the front, and can see that 'formality', 'criminality' and 'paucity' share subword units such as 'al' and 'ity'.\n",
        "\n",
        "You can access many pretrained models using the Gensim downloader. Using the cells below, you can try out some of the different models available through Gensim. Along with `word2vec` and `FastText`, Gensim also supports `Glove` and `Doc2Vec` models.\n",
        "\n",
        "**NB:** These trained models are very large, and will take a while to download. You may wish to download this notebook and execute the cells below on your own machine, in case Google kicks you out of the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rldVE5pmvKl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b55dc7-378e-4642-8491-13a5221accc0"
      },
      "source": [
        "# See what models are on offer\n",
        "print(list(api.info()['models'].keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### word2vec-google-news-300"
      ],
      "metadata": {
        "id": "UuEeDB7Tz2Mr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMTAxneZ7OaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0383c3-7a95-4043-dbbb-c7d48b32903d"
      },
      "source": [
        "# Optional! THIS WILL TAKE SOME TIME!!! (~15 min)\n",
        "# 300-dimensional word vectors trained on a huge dataset from Google News\n",
        "google_news_w2v = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = google_news_w2v.most_similar(negative=['soft'], positive=['hard','softer'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "069D1ZMCtpl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKwcm_7vKl-"
      },
      "source": [
        "# x is to Kenya as Canberra is to Australia\n",
        "google_news_w2v.most_similar(negative=['australia'],positive=['kenya','canberra'], topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### glove_wiki_gigaword_300"
      ],
      "metadata": {
        "id": "aleGhk_wz81B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download time ~ 4 min\n",
        "glove_wiki_gigaword_300 = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENhu-oq3p7No",
        "outputId": "e9f58681-7e58-473d-b217-f0efcbd84a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = glove_wiki_gigaword_300.most_similar(negative=['soft'], positive=['hard','softer'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "-0rnnIzqvn5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wikipedia_fasttext"
      ],
      "metadata": {
        "id": "aD6qfP7Y0Dwg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPZl6qzEvKl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e83a2ec-49f2-4a7c-986b-66f6ff75caf5"
      },
      "source": [
        "# Optional! THIS WILL TAKE SOME TIME!!! (download time ~10 minutes)\n",
        "# Facebook's own FastText vectors, trained on Wikipedia\n",
        "wikipedia_fasttext = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = wikipedia_fasttext.most_similar(negative=['soft'], positive=['hard','softer'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "QUxXP8aDvv0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4xj0CGQ7iFg"
      },
      "source": [
        "# x is to Wharton as London is to Dickens\n",
        "analogous_words = wikipedia_fasttext.most_similar(negative=['paris'],positive=['france','london'], topn=10)\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogous_words = glove_wiki_gigaword_300.most_similar(negative=['soft'], positive=['hard','softer'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "metadata": {
        "id": "JnNiRKt3vr1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4gsyd3-_8eN"
      },
      "source": [
        "# try your own examples..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqVFfYx878cA"
      },
      "source": [
        "## Step 5. Training models in other languages (Armenian, Georgian, Ukrainian ...)\n",
        "\n",
        "Now let's train a model on our own corpus. You can try to create Armenian / Ukrainian / Russian / German / French word vector model.\n",
        "\n",
        "Armenian, Ukrainian, Russian plaintext wikipedias are available at:\n",
        "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2735#\n",
        "\n",
        "We can use the same size ~15 Million Words, (200MB) of these corpora to make sure the model is built within a reasonable time; however, when you have more time, you can try a more complete corpus.\n",
        "\n",
        "It is possible to download/unzip/upload the files manually, however, it is faster to automatically load them directly from Heidelberg server space https://heibox.uni-heidelberg.de/d/42a07c9e95774e099a81/  into your colab environment. To do this run the cells 5a or 5b, or 5c, or 5d, or 5e -- depending on which language you prefer -- to download the data. Then skip other language and continue with Step5 -- building the model for your preferred language from the data you downloaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uwDxwRTSnqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eb32b2-9ed8-4c24-bc79-a1c725dd1628"
      },
      "source": [
        "!rm /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt\n",
        "!rm index.html?dl=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt': No such file or directory\n",
            "rm: cannot remove 'index.html?dl=1': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKKHYl4dFFcT"
      },
      "source": [
        "### 5c German - works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9i_seazFN9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f8220c-1f2f-4cdc-a85a-ceb2bdcf11aa"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/ca0a085347c34563a6e6/?dl=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 16:40:52--  https://heibox.uni-heidelberg.de/f/ca0a085347c34563a6e6/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/0040f62a-ead1-42a2-9dff-1f1edfd7debf/de8.txt [following]\n",
            "--2024-06-09 16:40:52--  https://heibox.uni-heidelberg.de/seafhttp/files/0040f62a-ead1-42a2-9dff-1f1edfd7debf/de8.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120000000 (114M) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 114.44M  14.9MB/s    in 7.3s    \n",
            "\n",
            "2024-06-09 16:41:00 (15.6 MB/s) - ‘index.html?dl=1’ saved [120000000/120000000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m13CwxeEH9dk"
      },
      "source": [
        "!cp index.html?dl=1 /usr/local/lib/python3.10/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBO_zgLb8vJm"
      },
      "source": [
        "!head --lines=15 /usr/local/lib/python3.10/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPZo393WEWGW"
      },
      "source": [
        "### Or: 5a Armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLc70OaADDsY"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/ffe527ed3d1e4b4cb43a/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj1swlOVH4J9"
      },
      "source": [
        "!cp index.html?dl=1 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASwFiA0tPfBn"
      },
      "source": [
        "!head --lines=15 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfLTw8mHGUA6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teve3mqpEdgG"
      },
      "source": [
        "### Or: 5b Ukrainian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz-lMC8ZEgTQ"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/676bef3a6db8482e9665/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kw3XZQPH47I"
      },
      "source": [
        "!cp index.html?dl=1 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARd-3RfY8mw5"
      },
      "source": [
        "!head --lines=15 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWDlNugkFOZP"
      },
      "source": [
        "### Or: 5d French"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL18LA1PFRAM"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/5cdb485efd4046f2a457/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BorzWO26H-Gx"
      },
      "source": [
        "!cp index.html?dl=1 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RushLVUX8w5k"
      },
      "source": [
        "!head --lines=15 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vgYZ0IfGY2l"
      },
      "source": [
        "### Or: 5f Georgian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imZY_tHOGc-m"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/fa3509d869b949459f91/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZZvDBTq3E8f"
      },
      "source": [
        "Lemmatized corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "767DwIll3IP6"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/b381c458ad5e4773a77d/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq2qM74k3Wfe"
      },
      "source": [
        "!cp index.html?dl=1 wiki_geo_lem.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUAVq_W3fqU"
      },
      "source": [
        "!head --lines=10 wiki_geo_lem.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDSkZErA3s8Z"
      },
      "source": [
        "FIn = open(\"wiki_geo_lem.txt\", 'r')\n",
        "FOut = open('wiki_geo_l.txt', 'w')\n",
        "\n",
        "i = 0\n",
        "for SLine in FIn:\n",
        "  i = i + 1\n",
        "  SLine = SLine.strip() + ' '\n",
        "  if i % 10000 == 0:\n",
        "    SLine += '\\n'\n",
        "\n",
        "  FOut.write(SLine)\n",
        "\n",
        "FOut.flush()\n",
        "FOut.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMGSnTHV5WyM"
      },
      "source": [
        "!head --lines=10 wiki_geo_l.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9_T1Dwz5sD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59125cfd-3027-4ed7-b6a3-e694e42e9049"
      },
      "source": [
        "!wc wiki_geo_lem.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 18054750  18054749 329202500 wiki_geo_lem.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrWmuRyEGk__"
      },
      "source": [
        "!cp index.html?dl=1 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg5rl4vM6QjO"
      },
      "source": [
        "Alternatively, copy lemmatized corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcDakluj6C2G"
      },
      "source": [
        "!cp wiki_geo_l.txt /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beg4vzqFGono"
      },
      "source": [
        "!head --lines=15 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AJ9guGSNkyw"
      },
      "source": [
        "### Stage6: Training own model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCj_-BYcOe7A"
      },
      "source": [
        "optional: to clean disk space, we remove the downloaded file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnO8k3CWOdqK"
      },
      "source": [
        "# optional: to clean disk space, we remove the downloaded file\n",
        "!rm index.html\\?dl\\=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AAzBAWaPQZB"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('myOwnLangText8.txt')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9VFjEuyQQ1u"
      },
      "source": [
        "import gensim.models\n",
        "corpusOwn = MyCorpus()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DARUsusgQyWO"
      },
      "source": [
        "optional: examining what is in the corpus after standard preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tLeaX0rQiD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed7a2a4-5784-4c5f-ed08-4ed63cd40c9d"
      },
      "source": [
        "# Optional: Examining our corpus format\n",
        "type(corpusOwn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.MyCorpus"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzwUYw_QxYD"
      },
      "source": [
        "dataOwn = [d for d in corpusOwn]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YWSko1SRLJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eda3ac2-373a-4191-956d-596f8d069597"
      },
      "source": [
        "print(len(dataOwn))\n",
        "print(dataOwn[0])\n",
        "print(len(dataOwn[0]))\n",
        "print(dataOwn[4])\n",
        "print(len(dataOwn[4]))\n",
        "print(dataOwn[5])\n",
        "print(len(dataOwn[5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "617267\n",
            "['alan', 'smithee']\n",
            "2\n",
            "['alternative', 'schreibweisen', 'sind', 'unter', 'anderem', 'die', 'allen', 'smithee', 'sowie', 'alan', 'smythee', 'und', 'adam', 'smithee', 'auch', 'zwei', 'teilweise', 'asiatisch', 'anmutende', 'schreibweisen', 'alan', 'smi', 'thee', 'und', 'sumishii', 'aran', 'gehören', 'so', 'die', 'internet', 'movie', 'database', 'dazu']\n",
            "33\n",
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468wJ6YkVNad"
      },
      "source": [
        "... Initialising global parameters for our modelL vector size, collocation window, skip-grams, negative sampling...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Eep9wyHh9d"
      },
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xObhQQ2SVBDn"
      },
      "source": [
        "my_vector_size = 100 # Dimensionality of the word vectors\n",
        "window = 5 # How many words either side? (5 = 5 context words either side, i.e. 10 context words in total)\n",
        "use_skip_gram = 1 # If you set this to 0, then it will create a 'continuous bag of words' model instead\n",
        "use_softmax = 0 # If you set this to 1, then hierarchical softmax will be used instead of negative sampling\n",
        "negative_samples = 5 # How many incorrect answers to generate per correct answer when negative sampling\n",
        "\n",
        "modelOwn = Word2Vec(\n",
        "    vector_size=my_vector_size,\n",
        "    window=window,\n",
        "    sg=use_skip_gram,\n",
        "    hs=use_softmax,\n",
        "    negative=negative_samples\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_QILgoIVeea"
      },
      "source": [
        "... this cell may run for ~2 min or so..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaecmdkPVLCh"
      },
      "source": [
        "modelOwn.build_vocab(corpusOwn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdwCKEVVnGv"
      },
      "source": [
        "THIS MAY TAKE LONG!!! ... training the model may take 9 to 15 minutes... (just grab a cup of coffee or a sandwich while you are waiting... You may try chaning the number of epochs; if the number is lower the training is faster, but the quality may be lower..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrvDUrOJVxOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988c0cee-da4b-425b-96fc-9c9226772495"
      },
      "source": [
        "modelOwn.train(corpusOwn, epochs=5, total_examples=modelOwn.corpus_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56971850, 75507725)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrJqAeUWWmL"
      },
      "source": [
        "Now we copy word vectors and remove the model from memory (just to free up the resources...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2GbyMw5WVZd"
      },
      "source": [
        "word_vectors_own = modelOwn.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjLyitW5Whff"
      },
      "source": [
        "del modelOwn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ipjoSmWtOR"
      },
      "source": [
        "Now we can examine the output of our word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyKlhpZJ6oG3"
      },
      "source": [
        "### German examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO9GGM5K6kup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2057637-9971-4f87-f633-4949574d640c"
      },
      "source": [
        "# DE examples\n",
        "# See the word vector for a particular word\n",
        "vector = word_vectors_own['banane']\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.10075621 -0.12602118 -0.09882525  0.07208958  0.0338595  -0.1839196\n",
            "  0.279739    0.31493902 -0.07504811 -0.18543933 -0.14300443 -0.08045007\n",
            " -0.20416714  0.09101471  0.03044987 -0.22884238  0.05614935  0.05093222\n",
            "  0.24569729 -0.24122925  0.06150239 -0.20853686  0.16781837 -0.15910992\n",
            " -0.24888885  0.14393839  0.058139   -0.07789802 -0.0904852  -0.10539671\n",
            "  0.22656076 -0.22272146  0.1546064   0.17589416  0.01947307  0.2050054\n",
            " -0.08090865 -0.17477393 -0.22255301  0.10397669  0.17486668 -0.27339187\n",
            "  0.2600213   0.16386129 -0.22488172 -0.14953832 -0.06997634 -0.17026299\n",
            " -0.0010755   0.19792816  0.3408265   0.05118159 -0.33898032 -0.03472055\n",
            " -0.29269937  0.03965892  0.04215126  0.19848008 -0.06039856 -0.1654338\n",
            " -0.03749323 -0.09299678  0.02412619  0.21416458  0.1188198   0.38227263\n",
            " -0.04517859  0.1417562  -0.31381208  0.2106447   0.00822109 -0.13669954\n",
            "  0.4710996   0.17543544  0.12046697 -0.02408728  0.09891791 -0.08062794\n",
            "  0.009671    0.2026222  -0.04359178 -0.31278443  0.01184188  0.36761302\n",
            "  0.02077505  0.1416185   0.10241165  0.0817978   0.2884969   0.04498396\n",
            "  0.07561106  0.04933546 -0.1857667   0.27813563  0.31002596  0.23424911\n",
            "  0.32822376 -0.10521352  0.03321151 -0.13368946]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irvedMmO62EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ae6752-bb32-44b2-9bd4-d7b547ce28ba"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors_own.most_similar('universität', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('hochschule', 0.8408931493759155)\n",
            "('fakultät', 0.7962464690208435)\n",
            "('universitäten', 0.7852887511253357)\n",
            "('ehrendoktor', 0.7707276344299316)\n",
            "('university', 0.7700812220573425)\n",
            "('volluniversität', 0.7546650767326355)\n",
            "('hochschulen', 0.7529446482658386)\n",
            "('fachhochschule', 0.7368623614311218)\n",
            "('sorbonne', 0.7329705953598022)\n",
            "('kunsthochschule', 0.7321016192436218)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfmC7307DBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c836aec-f51b-4724-e052-ac62b1db080c"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors_own.most_similar('rot', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('blau', 0.9078530669212341)\n",
            "('grün', 0.8994275331497192)\n",
            "('gelb', 0.8767167925834656)\n",
            "('schwarz', 0.8723684549331665)\n",
            "('stadtfarben', 0.8272213935852051)\n",
            "('stadtflagge', 0.8210878372192383)\n",
            "('landesfarben', 0.811372697353363)\n",
            "('silbern', 0.7903183698654175)\n",
            "('weiß', 0.7887179851531982)\n",
            "('gestreift', 0.7856178879737854)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ5fp9mI7G0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0815ff-bcee-4486-bc96-69d537f9bf8d"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors_own.most_similar('brüssel', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('genf', 0.8063685297966003)\n",
            "('amsterdam', 0.7932760119438171)\n",
            "('paris', 0.7852495312690735)\n",
            "('madrid', 0.7727000713348389)\n",
            "('lüttich', 0.7647874355316162)\n",
            "('straßburg', 0.7594782710075378)\n",
            "('charleroi', 0.7566460967063904)\n",
            "('lille', 0.7544008493423462)\n",
            "('toulouse', 0.7536197900772095)\n",
            "('stockholm', 0.7521782517433167)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prPdl2ki7JLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52a4f22-76cb-4dcb-f0c2-b24adc36a526"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors_own.most_similar('deutschland', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('österreich', 0.8338862061500549)\n",
            "('bundesrepublik', 0.8042684197425842)\n",
            "('schweiz', 0.7832845449447632)\n",
            "('ostdeutschland', 0.7801434993743896)\n",
            "('westdeutschland', 0.7517196536064148)\n",
            "('belgien', 0.725387692451477)\n",
            "('europa', 0.7193728089332581)\n",
            "('frankreich', 0.7093033194541931)\n",
            "('lettland', 0.70259028673172)\n",
            "('verkehrsclub', 0.698092520236969)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxHXJ_k7Lxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6120f7-605a-4338-f6a7-32a6ec9d6070"
      },
      "source": [
        "# See which words are closest to a given word in the vector space\n",
        "similar_words = word_vectors_own.most_similar('komisch', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('langweilig', 0.8917211294174194)\n",
            "('sympathisch', 0.8823737502098083)\n",
            "('witzig', 0.8816604614257812)\n",
            "('banal', 0.8811015486717224)\n",
            "('wunderbar', 0.8784763813018799)\n",
            "('wahrhaftig', 0.8775321245193481)\n",
            "('hassen', 0.8730990290641785)\n",
            "('anschauen', 0.8728605508804321)\n",
            "('tragisch', 0.8708015084266663)\n",
            "('großartig', 0.86897873878479)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqplaUng7N5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100cfd7e-122a-4df6-d636-d7fccc490dfd"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors_own.most_similar(negative=['mann'], positive=['könig','frau'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('krönung', 0.6899225115776062)\n",
            "('königin', 0.6776207685470581)\n",
            "('gemahlin', 0.671101450920105)\n",
            "('ottos', 0.6473620533943176)\n",
            "('regentin', 0.6402062773704529)\n",
            "('theresia', 0.6371099352836609)\n",
            "('prinzessin', 0.6355440616607666)\n",
            "('thronfolger', 0.6326956748962402)\n",
            "('sigismund', 0.6317800879478455)\n",
            "('isabella', 0.630694568157196)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWhfjdjU7QEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ea754e-75d1-44e6-89e2-1112718fdfac"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors_own.most_similar(negative=['könig'], positive=['königin','mann'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('großmutter', 0.6486426591873169)\n",
            "('zuviel', 0.6462733149528503)\n",
            "('frau', 0.6370766162872314)\n",
            "('ellen', 0.6279744505882263)\n",
            "('wußte', 0.6237425208091736)\n",
            "('elizabeth', 0.6159384250640869)\n",
            "('schauspielerin', 0.6155344247817993)\n",
            "('anne', 0.6154522895812988)\n",
            "('alice', 0.6134677529335022)\n",
            "('tante', 0.6102522611618042)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXs0DTXE7UYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea6f865-5712-40a5-f7a4-b3814ea40a58"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors_own.most_similar(negative=['mutter'], positive=['vater','tochter'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('sohn', 0.8599680662155151)\n",
            "('bruder', 0.7702828049659729)\n",
            "('großvater', 0.7331442832946777)\n",
            "('cousin', 0.7274137139320374)\n",
            "('halbbruder', 0.7147159576416016)\n",
            "('ehefrau', 0.7091336846351624)\n",
            "('schwiegersohn', 0.7005618810653687)\n",
            "('erbin', 0.6972846388816833)\n",
            "('enkel', 0.691847026348114)\n",
            "('neffe', 0.6910774111747742)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRHwRsT_7YLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3fa385-0fa2-407b-fe88-fc738a71d40a"
      },
      "source": [
        "# Compute analogous words\n",
        "# E.g. x is to queen as man is to king => x = v('queen') + v('man') - v('king')\n",
        "analogous_words = word_vectors_own.most_similar(negative=['mutter'], positive=['vater','mädchen'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('werthers', 0.6393195986747742)\n",
            "('jungen', 0.6264751553535461)\n",
            "('jugendliche', 0.595009446144104)\n",
            "('lehrer', 0.5943997502326965)\n",
            "('hauslehrern', 0.5909024477005005)\n",
            "('hund', 0.5899500250816345)\n",
            "('prostituierte', 0.5867388844490051)\n",
            "('musizieren', 0.5775508284568787)\n",
            "('kleinkinder', 0.5749905109405518)\n",
            "('engel', 0.5749112367630005)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzSy-z-07Yy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8117c974-7662-4220-c4a7-a9cec75f633e"
      },
      "source": [
        "# Compute analogous words\n",
        "# x is to daughter as people is to person (plural + daughter)\n",
        "analogous_words = word_vectors_own.most_similar(negative=['person'], positive=['menschen','tochter'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('heirateten', 0.5747276544570923)\n",
            "('geboren', 0.5573669672012329)\n",
            "('mutter', 0.5540222525596619)\n",
            "('geborene', 0.5523591637611389)\n",
            "('kinder', 0.5452825427055359)\n",
            "('witwe', 0.5435317158699036)\n",
            "('heiratete', 0.5411205887794495)\n",
            "('unehelich', 0.5395071506500244)\n",
            "('schwester', 0.5393198728561401)\n",
            "('großmutter', 0.5384270548820496)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWv3si3k7bK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684cf679-8da5-452a-b12e-f584d3a64c73"
      },
      "source": [
        "analogous_words = word_vectors_own.most_similar(negative=['weich'], positive=['hart','weicher'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('kalte', 0.7127009630203247)\n",
            "('milder', 0.6817352175712585)\n",
            "('polarluft', 0.6750123500823975)\n",
            "('warmer', 0.6746388077735901)\n",
            "('bescheidener', 0.6650974750518799)\n",
            "('katabatischer', 0.6614716649055481)\n",
            "('südschweiz', 0.6606355309486389)\n",
            "('zäh', 0.6599506139755249)\n",
            "('unangenehm', 0.6589765548706055)\n",
            "('humides', 0.6531760096549988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f-JPmP0JILh"
      },
      "source": [
        "### Georgian examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhbX0fpdH90R"
      },
      "source": [
        "# See the word vector for a particular word -- 'world'\n",
        "vector = word_vectors_own['სამყარო']\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0dPtZK2IkpT"
      },
      "source": [
        "# See which words are closest to a given word in the vector space = 'world'\n",
        "similar_words = word_vectors_own.most_similar('არსება', topn=30)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLuAjAlcIrpi"
      },
      "source": [
        "# blue\n",
        "similar_words = word_vectors_own.most_similar('ლურჯი', topn=30)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGuQWHznIuXV"
      },
      "source": [
        "# france\n",
        "similar_words = word_vectors_own.most_similar('საფრანგეთი', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzFct-O4IzYr"
      },
      "source": [
        "# x is to king as woman is to man\n",
        "analogous_words = word_vectors_own.most_similar(negative=['კაცი'], positive=['მეფე','ქალი'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q60fDwHXI3RL"
      },
      "source": [
        "# x is to king as woman is to man\n",
        "# x = small + biggest - big\n",
        "# 𝑥−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑠𝑚𝑎𝑙𝑙′)=𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔𝑔𝑒𝑠𝑡′)−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔′)\n",
        "# ∴ 𝑥=𝑣𝑒𝑐𝑡𝑜𝑟(′𝑠𝑚𝑎𝑙𝑙′)+𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔𝑔𝑒𝑠𝑡′)−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔′)\n",
        "# ∴ 𝑥=𝑣𝑒𝑐𝑡𝑜𝑟(′փոքր′)+𝑣𝑒𝑐𝑡𝑜𝑟(′ամենամեծը′)−𝑣𝑒𝑐𝑡𝑜𝑟(′մեծ′)\n",
        "analogous_words = word_vectors_own.most_similar(negative=['პირი'], positive=['ხალხი','ქალიშვილი'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czmhvtc-JB4C"
      },
      "source": [
        "# try your own examples... (also -- morphology?)\n",
        "# # x is to daughter as people is to person (plural + daughter)\n",
        "# analogous_words = word_vectors.most_similar(negative=['person'], positive=['people','daughter'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUFzOu3eIrGr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvj5B2qdJDt6"
      },
      "source": [
        "### Armenian examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUVomonLWzHM"
      },
      "source": [
        "# See the word vector for a particular word -- 'world'\n",
        "vector = word_vectors_own['աշխարհը']\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga4LQhZJXmYV"
      },
      "source": [
        "# See which words are closest to a given word in the vector space = 'world'\n",
        "similar_words = word_vectors_own.most_similar('աշխարհը', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ECDUd8AXsF0"
      },
      "source": [
        "# blue\n",
        "similar_words = word_vectors_own.most_similar('կապույտ', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKAl9NVdX8SM"
      },
      "source": [
        "# france\n",
        "similar_words = word_vectors_own.most_similar('ֆրանսիա', topn=10)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "492wftrxZRhR"
      },
      "source": [
        "# x is to king as woman is to man\n",
        "analogous_words = word_vectors_own.most_similar(negative=['մարդ'], positive=['թագավոր','կին'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HOugAyWbYCE"
      },
      "source": [
        "# x is to king as woman is to man\n",
        "# x = small + biggest - big\n",
        "# 𝑥−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑠𝑚𝑎𝑙𝑙′)=𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔𝑔𝑒𝑠𝑡′)−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔′)\n",
        "# ∴ 𝑥=𝑣𝑒𝑐𝑡𝑜𝑟(′𝑠𝑚𝑎𝑙𝑙′)+𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔𝑔𝑒𝑠𝑡′)−𝑣𝑒𝑐𝑡𝑜𝑟(′𝑏𝑖𝑔′)\n",
        "# ∴ 𝑥=𝑣𝑒𝑐𝑡𝑜𝑟(′փոքր′)+𝑣𝑒𝑐𝑡𝑜𝑟(′ամենամեծը′)−𝑣𝑒𝑐𝑡𝑜𝑟(′մեծ′)\n",
        "analogous_words = word_vectors_own.most_similar(negative=['մեծ'], positive=['ամենամեծը','փոքր'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz1h1O-MaY4J"
      },
      "source": [
        "# try your own examples... (also -- morphology?)\n",
        "# # x is to daughter as people is to person (plural + daughter)\n",
        "# analogous_words = word_vectors.most_similar(negative=['person'], positive=['people','daughter'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}